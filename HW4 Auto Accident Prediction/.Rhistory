dplyr::select(-c(HOMEKIDS, KIDSDRIV, EDUCATION, JOB, CAR_TYPE))#, TARGET_AMT
transf_v1_lm <- lm(TARGET_AMT~., data = train_transformed)
PT <- powerTransform(as.formula(transf_v1_lm$call), family = "bcnPower", data = train_transformed)
PT
PT
a <- car::bcnPower(train_transformed$TARGET_AMT, lambda = PT$lambda, gamma = PT$gamma)
a <- car::bcnPower(train_transformed$TARGET_AMT, lambda = PT$lambda, gamma = PT$gamma)
a
hist(a)
a <- car::bcnPower(train_transformed$TARGET_AMT, lambda = PT$lambda, gamma = PT$gamma)
hist(a)
hist(a, bins = 20)
hist(a, bin = 20)
hist(a, breaks = 20)
hist(a, breaks = 40)
summary(a )
(PT <- car::powerTransform(as.formula(transf_v1_lm$call), family = "bcnPower", data = train_transformed))
hist(a, breaks = 40)
a <- car::bcnPower(train_transformed$TARGET_AMT, lambda = PT$lambda, gamma = PT$gamma)
a
summary(a )
b <- bcnPowerInverse(a, PT$lambda, PT$gamma)
b <- car::bcnPowerInverse(a, PT$lambda, PT$gamma)
load_install <- function(pkg){
# Load packages. Install them if needed.
# CODE SOURCE: https://gist.github.com/stevenworthington/3178163
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}
# required packages
packages <- c("prettydoc","tidyverse", "caret", "pROC", "DT", "knitr", "ggthemes", "Hmisc", "psych", "corrplot", "reshape2", "car", "MASS", "ResourceSelection", "boot", "tinytex", "devtools", "VIM", "GGally", "missForest", "DMwR", "nortest")
#install_version("rmarkdown",version=1.8)
#excute function and display the loaded packages
table(load_install(packages))
#data.frame(load_install(packages))
#load_install(packages)
b <- car::bcnPowerInverse(a, PT$lambda, PT$gamma)
car::bcnPowerInverse(a, PT$lambda, PT$gamma)
b <- car::bcnPowerInverse(a, PT$lambda, PT$gamma)
(PT <- car::powerTransform(as.formula(transf_v1_lm$call), family = "bcnPower", data = train_transformed))
a <- car::bcnPower(train_transformed$TARGET_AMT, lambda = PT$lambda, gamma = PT$gamma)
a
train_transformed$TARGET_AMT^PT$lambda/PT$gamma
(PT <- car::powerTransform(as.formula(transf_v1_lm$call), family = "bcnPower", data = train_transformed))
(train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda
((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda
sum(((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda == a)
all.equal(((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda == a)
all.equal(((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda, a)
a
train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda
((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda
summary(a )
b <- car::bcnPowerInverse(a, PT$lambda, PT$gamma)
(PT <- car::powerTransform(as.formula(transf_v1_lm$call), family = "bcPower", data = train_transformed))
(PT <- car::powerTransform(as.formula(transf_v1_lm$call), family = "bcnPower", data = train_transformed))
b <- car::bcnPowerInverse(a, PT$lambda, PT$gamma)
b <- car::bcnPowerInverse(a, lambda = PT$lambda, gamma = PT$gamma)
b <- bcnPowerInverse(a, lambda = PT$lambda, gamma = PT$gamma)
summary(a)
b <- bcnPowerInverse(a, lambda = PT$lambda, gamma = PT$gamma)
a <- car::bcnPower(train_transformed$TARGET_AMT, lambda = PT$lambda, gamma = PT$gamma)
all.equal(((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda, a)
hist(a, breaks = 40)
(PT <- car::powerTransform(as.formula(transf_v1_lm$call), family = "bcnPower", data = train_transformed))
a <- car::bcnPower(train_transformed$TARGET_AMT, lambda = PT$lambda, gamma = PT$gamma)
b <- bcnPowerInverse(as.formula(transf_v1_lm$call), lambda = PT$lambda, gamma = PT$gamma)
# pairs(dplyr::select(select_if(train_transformed, is.numeric), -c(NOHOMEKIDS, NOKIDSDRIV, HASCOLLEGE, ISPROFESSIONAL, ISMINIVAN, CLM_FREQ, TARGET_FLAG)))
library(car)
# pairs(dplyr::select(select_if(train_transformed, is.numeric), -c(NOHOMEKIDS, NOKIDSDRIV, HASCOLLEGE, ISPROFESSIONAL, ISMINIVAN, CLM_FREQ, TARGET_FLAG)))
require(car)
# pairs(dplyr::select(select_if(train_transformed, is.numeric), -c(NOHOMEKIDS, NOKIDSDRIV, HASCOLLEGE, ISPROFESSIONAL, ISMINIVAN, CLM_FREQ, TARGET_FLAG)))
require(car)
# pairs(dplyr::select(select_if(train_transformed, is.numeric), -c(NOHOMEKIDS, NOKIDSDRIV, HASCOLLEGE, ISPROFESSIONAL, ISMINIVAN, CLM_FREQ, TARGET_FLAG)))
require(car)
bcnPowerInverse
b <- bcnPowerInverse(a, lambda = PT$lambda, gamma = PT$gamma)
all.equal(((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda, a)
sum(isTRUE(all.equal(((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda, a)))
isTRUE(all.equal(((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda, a))
sum(mapply(all.equal, ((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda, a))
sum(mapply(function(x) isTRUE(all.equal(x)), ((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda, a))
sum(mapply(function(x, y) isTRUE(all.equal(x, y)), ((train_transformed$TARGET_AMT+ PT$gamma)^PT$lambda-1)/PT$lambda, a))
ia <- (a * PT$lambda +  1)^(1/PT$lambda) - PT$gamma
ia
ia
ia <- (a * PT$lambda +  1)^(1/PT$lambda) - PT$gamma
ia
MASS::boxcox(transf_v1_lm)
MASS::boxcox(as.formula(transf_v1_lm$call))
MASS::boxcox(transf_v1_lm)
summary(ia)
summary(a)
summary(ia)
summary(train_transformed$TARGET_AMT)
summary(ia)
cor(train_transformed$TARGET_AMT, ia)
PT$gamma
ia <- (a * PT$lambda +  1)^(1/PT$lambda) #- PT$gamma
cor(train_transformed$TARGET_AMT, ia)
summary(a)
summary(train_transformed$TARGET_AMT)
summary(train_transformed$TARGET_AMT)
summary(ia)
ia <- (a * PT$lambda +  1)^(1/PT$lambda) - PT$gamma
ia <- (a * PT$lambda +  1)^(1/PT$lambda) - PT$gamma
cor(train_transformed$TARGET_AMT, ia)
summary(train_transformed$TARGET_AMT)
summary(train_transformed$TARGET_AMT)
summary(ia)
PT$gamma
train_transformed$TARGET_AMT <- car::bcnPower(train_transformed$TARGET_AMT, lambda = PT$lambda, gamma = PT$gamma)
train_transformed$TARGET_AMT
transf_var_lmod <- lm(sqrt.TARGET_AMT ~ .-TARGET_FLAG, data = train_transformed)
transf_var_lmod <- lm(TARGET_AMT ~ .-TARGET_FLAG, data = train_transformed)
n <- nrow(transf_var_lmod$model)
BIC_transf_var_lm <- step(transf_var_lmod, k = log(n), trace = 0)
summary(BIC_transf_var_lm)
# cooks_dist <- cooks.distance(BIC_transf_var_lm)
# cooks_dist_mean <- mean(cooks_dist)
# sum(cooks_dist > cooks_dist_mean * 2)
# plot(BIC_transf_var_lm)
# train_data[c(7691, 7072, 5389), ]
# sort(train_data$TARGET_AMT, decreasing = T)[1:4]
# psych::describe(BIC_transf_var_lm$model$sqrt.TARGET_AMT)
#evaluate performance & diagnostics
model_eval <- lm_evaluation(BIC_transf_var_lm)
model_diag <- lm_diagnotics(BIC_transf_var_lm)
lm_results <- rbind(lm_results, model_eval)
lm_results_diagnostics <- rbind(lm_results_diagnostics, model_diag)
kable(lm_results, digits = 3)
kable(lm_results_diagnostics, digits = 3)
BCN_inverse <- function(U, lambda, gamma){
(U * lambda +  1)^(1/lambda) - gamma
}
BCN_inverse(a)
BCN_inverse(a, lambda = PT$lambda, gamma = PT$gamma)
summary(BCN_inverse(a, lambda = PT$lambda, gamma = PT$gamma))
unlink('D621-Data-Mining/HW4 Auto Accident Prediction/HW4 Auto Accident Prediction_cache', recursive = TRUE)
unlink('D621-Data-Mining/HW4 Auto Accident Prediction/HW4 Auto Accident Prediction_cache', recursive = TRUE)
shapiro.test(BIC_transf_var_lm)
shapiro.test(BIC_transf_var_lm$residuals)
orig_var_glm <- glm(target ~ . , family = "binomial", data = imputed_train$ximp)
orig_var_glm <- glm(TARGET_FLAG ~ . , family = "binomial", data = dplyr::select(imputed_train$ximp, -TARGET_AMT))
summary(bk_elim_orig_vars <- backward_elimination(orig_var_glm))
backward_elimination <- function(lmod){
#performs backward elimination model selection
#removes variables until all remaining ones are stat-sig
removed_vars <- c()
removed_pvalues <- c()
#handles category variables
cat_levels <- unlist(lmod$xlevels)
cat_vars <- str_sub(names(cat_levels), 1, nchar(names(cat_levels)) - 1)
cat_var_df <- data.frame(cat_vars,
dummy_vars = str_c(cat_vars, cat_levels),
stringsAsFactors = F)
# checks for p-values > .05 execpt for the intercept
while (max(summary(lmod)$coefficients[2:length(summary(lmod)$coefficients[, 4]), 4]) > .05){
# find insignificant pvalue
pvalues <- summary(lmod)$coefficients[2:length(summary(lmod)$coefficients[, 4]), 4]
max_pvalue <- max(pvalues)
remove <- names(which.max(pvalues))
#if categorical dummy variable, remove the variable
dummy_var <- dplyr::filter(cat_var_df, dummy_vars == remove)
print(remove <- ifelse(nrow(dummy_var) > 0, dummy_var[, 1], remove))
removed_vars <- c(removed_vars, remove)
removed_pvalues <- c(removed_pvalues, max_pvalue)
# update model
lmod <- update(lmod, as.formula(paste0(".~.-`", remove, "`")))
}
print("Removed variables:")
print(kable(data.frame(removed_vars, removed_pvalues), digits = 3))
return(lmod)
}
orig_var_glm <- glm(TARGET_FLAG ~ . , family = "binomial", data = dplyr::select(imputed_train$ximp, -TARGET_AMT))
summary(bk_elim_orig_vars <- backward_elimination(orig_var_glm))
backward_elimination <- function(lmod){
#performs backward elimination model selection
#removes variables until all remaining ones are stat-sig
removed_vars <- c()
removed_pvalues <- c()
#handles category variables
cat_levels <- unlist(lmod$xlevels)
cat_vars <- str_sub(names(cat_levels), 1, nchar(names(cat_levels)) - 1)
cat_var_df <- data.frame(cat_vars,
dummy_vars = str_c(cat_vars, cat_levels),
stringsAsFactors = F)
# checks for p-values > .05 execpt for the intercept
while (max(summary(lmod)$coefficients[2:length(summary(lmod)$coefficients[, 4]), 4]) > .05){
# find insignificant pvalue
pvalues <- summary(lmod)$coefficients[2:length(summary(lmod)$coefficients[, 4]), 4]
max_pvalue <- max(pvalues)
remove <- names(which.max(pvalues))
#if categorical dummy variable, remove the variable
dummy_var <- dplyr::filter(cat_var_df, dummy_vars == remove)
print(remove <- ifelse(nrow(dummy_var) > 0, dummy_var[, 1], remove))
removed_vars <- c(removed_vars, remove)
removed_pvalues <- c(removed_pvalues, max_pvalue)
# update model
lmod <- update(lmod, as.formula(paste0(".~.-`", remove, "`")))
}
print("Removed variables:")
print(kable(data.frame(removed_vars, removed_pvalues), digits = 3))
return(lmod)
}
orig_var_glm <- glm(TARGET_FLAG ~ . , family = "binomial", data = dplyr::select(imputed_train$ximp, -TARGET_AMT))
summary(bk_elim_orig_vars <- backward_elimination(orig_var_glm))
glm_performance <- function(model) {
### Summarizes the model's key statistics
### References: https://www.r-bloggers.com/predicting-creditability-using-logistic-regression-in-r-cross-validating-the-classifier-part-2-2/
### https://www.rdocumentation.org/packages/boot/versions/1.3-20/topics/cv.glm
### https://www.rdocumentation.org/packages/ResourceSelection/versions/0.3-1/topics/hoslem.test
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
df_summary <- data.frame(
model_name = deparse(substitute(model)),
n_vars = length(coef(model)) - 1,
model_pvalue = formatC(pchisq(model$null.deviance - model$deviance, 1, lower=FALSE), format = "e", digits = 2),
residual_deviance = model$deviance,
H_L_pvalue = hoslem.test(model$y, fitted(model))$p.value,
VIF_gt_4 = sum(car::vif(model) > 4),
LOOCV_accuracy = 1 - cv.glm(model$model, model, cost = cost)$delta[1],
AUC = as.numeric(pROC::roc(model$y, fitted(model))$auc)
)
}
glm_models <- glm_performance(bk_elim_orig_vars)
summary(bk_elim_orig_vars <- backward_elimination(orig_var_glm))
glm_models <- glm_performance(bk_elim_orig_vars)
summary(bk_elim_orig_vars <- backward_elimination(orig_var_glm))
glm_models <- glm_performance(bk_elim_orig_vars)
backward_elimination <- function(lmod){
#performs backward elimination model selection
#removes variables until all remaining ones are stat-sig
removed_vars <- c()
removed_pvalues <- c()
#handles category variables
cat_levels <- unlist(lmod$xlevels)
cat_vars <- str_sub(names(cat_levels), 1, nchar(names(cat_levels)) - 1)
cat_var_df <- data.frame(cat_vars,
dummy_vars = str_c(cat_vars, cat_levels),
stringsAsFactors = F)
# checks for p-values > .05 execpt for the intercept
while (max(summary(lmod)$coefficients[2:length(summary(lmod)$coefficients[, 4]), 4]) > .05){
# find insignificant pvalue
pvalues <- summary(lmod)$coefficients[2:length(summary(lmod)$coefficients[, 4]), 4]
max_pvalue <- max(pvalues)
remove <- names(which.max(pvalues))
#if categorical dummy variable, remove the variable
dummy_var <- dplyr::filter(cat_var_df, dummy_vars == remove)
print(remove <- ifelse(nrow(dummy_var) > 0, dummy_var[, 1], remove))
removed_vars <- c(removed_vars, remove)
removed_pvalues <- c(removed_pvalues, max_pvalue)
# update model
lmod <- update(lmod, as.formula(paste0(".~.-`", remove, "`")))
}
print("Removed variables:")
print(kable(data.frame(removed_vars, removed_pvalues), digits = 3))
return(lmod)
}
orig_var_glm <- glm(TARGET_FLAG ~ . , family = "binomial", data = dplyr::select(imputed_train$ximp, -TARGET_AMT))
summary(bk_elim_orig_vars <- backward_elimination(orig_var_glm))
glm_performance <- function(model) {
### Summarizes the model's key statistics
### References: https://www.r-bloggers.com/predicting-creditability-using-logistic-regression-in-r-cross-validating-the-classifier-part-2-2/
### https://www.rdocumentation.org/packages/boot/versions/1.3-20/topics/cv.glm
### https://www.rdocumentation.org/packages/ResourceSelection/versions/0.3-1/topics/hoslem.test
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
df_summary <- data.frame(
model_name = deparse(substitute(model)),
n_vars = length(coef(model)) - 1,
model_pvalue = formatC(pchisq(model$null.deviance - model$deviance, 1, lower=FALSE), format = "e", digits = 2),
residual_deviance = model$deviance,
H_L_pvalue = hoslem.test(model$y, fitted(model))$p.value,
VIF_gt_4 = sum(car::vif(model) > 4),
LOOCV_accuracy = 1 - boot::cv.glm(model$model, model, cost = cost)$delta[1],
AUC = as.numeric(pROC::roc(model$y, fitted(model))$auc)
)
return(df_summary)
}
glm_models <- glm_performance(bk_elim_orig_vars)
glm_models <- glm_performance(bk_elim_orig_vars)
summary(bk_elim_orig_vars <- backward_elimination(orig_var_glm))
glm_performance <- function(model) {
### Summarizes the model's key statistics
### References: https://www.r-bloggers.com/predicting-creditability-using-logistic-regression-in-r-cross-validating-the-classifier-part-2-2/
### https://www.rdocumentation.org/packages/boot/versions/1.3-20/topics/cv.glm
### https://www.rdocumentation.org/packages/ResourceSelection/versions/0.3-1/topics/hoslem.test
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
df_summary <- data.frame(
model_name = deparse(substitute(model)),
n_vars = length(coef(model)) - 1,
model_pvalue = formatC(pchisq(model$null.deviance - model$deviance, 1, lower=FALSE), format = "e", digits = 2),
residual_deviance = model$deviance,
H_L_pvalue = hoslem.test(model$y, fitted(model))$p.value,
VIF_gt_4 = sum(car::vif(model) > 4),
LOOCV_accuracy = 1 - boot::cv.glm(model$model, model, cost = cost)$delta[1],
AUC = as.numeric(pROC::roc(model$y, fitted(model))$auc)
)
return(df_summary)
}
glm_models <- glm_performance(bk_elim_orig_vars)
glm_models <- glm_performance(bk_elim_orig_vars)
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
model = bk_elim_orig_vars
n_vars = length(coef(model)) - 1
n_vars
model_pvalue = formatC(pchisq(model$null.deviance - model$deviance, 1, lower=FALSE), format = "e", digits = 2)
model_pvalue
residual_deviance = model$deviance
residual_deviance
H_L_pvalue = hoslem.test(model$y, fitted(model))$p.value,
H_L_pvalue = hoslem.test(model$y, fitted(model))$p.value
H_L_pvalue
VIF_gt_4 = sum(car::vif(model) > 4)
VIF_gt_4
LOOCV_accuracy = 1 - boot::cv.glm(model$model, model, cost = cost)$delta[1]
AUC = as.numeric(pROC::roc(model$y, fitted(model))$auc)
AUC
bk_elim_orig_vars_roc <- roc(base_model$y, fitted(base_model))
bk_elim_orig_vars_roc <- roc(bk_elim_orig_vars$y, fitted(bk_elim_orig_vars))
plot(bk_elim_orig_vars_roc, legacy.axes = T, main = "Model ROCs", col = "gray", xaxs = "i", yaxs = "i")
bk_elim_orig_vars_roc <- roc(bk_elim_orig_vars$y, fitted(bk_elim_orig_vars))
plot(bk_elim_orig_vars_roc, legacy.axes = T, main = "Model ROCs", col = "gray", xaxs = "i", yaxs = "i")
#https://web.archive.org/web/20160407221300/http://metaoptimize.com:80/qa/questions/988/simple-explanation-of-area-under-the-roc-curve
1 - boot::cv.glm(bk_elim_orig_vars$model, bk_elim_orig_vars, cost = cost)$delta[1]
bk_elim_orig_vars$model
1 - boot::cv.glm(bk_elim_orig_vars$model, bk_elim_orig_vars, cost = cost, K = 816)$delta[1]
1 - boot::cv.glm(bk_elim_orig_vars$model, bk_elim_orig_vars, cost = cost, K = 81)$delta[1]
1 - boot::cv.glm(bk_elim_orig_vars$model, bk_elim_orig_vars, cost = cost, K = 100)$delta[1]
backward_elimination <- function(lmod){
#performs backward elimination model selection
#removes variables until all remaining ones are stat-sig
removed_vars <- c()
removed_pvalues <- c()
#handles category variables
cat_levels <- unlist(lmod$xlevels)
cat_vars <- str_sub(names(cat_levels), 1, nchar(names(cat_levels)) - 1)
cat_var_df <- data.frame(cat_vars,
dummy_vars = str_c(cat_vars, cat_levels),
stringsAsFactors = F)
# checks for p-values > .05 execpt for the intercept
while (max(summary(lmod)$coefficients[2:length(summary(lmod)$coefficients[, 4]), 4]) > .05){
# find insignificant pvalue
pvalues <- summary(lmod)$coefficients[2:length(summary(lmod)$coefficients[, 4]), 4]
max_pvalue <- max(pvalues)
remove <- names(which.max(pvalues))
#if categorical dummy variable, remove the variable
dummy_var <- dplyr::filter(cat_var_df, dummy_vars == remove)
print(remove <- ifelse(nrow(dummy_var) > 0, dummy_var[, 1], remove))
#record the removed variables
removed_vars <- c(removed_vars, remove)
removed_pvalues <- c(removed_pvalues, max_pvalue)
# update model
lmod <- update(lmod, as.formula(paste0(".~.-`", remove, "`")))
}
print("Removed variables:")
print(kable(data.frame(removed_vars, removed_pvalues), digits = 3))
return(lmod)
}
orig_var_glm <- glm(TARGET_FLAG ~ . , family = "binomial", data = dplyr::select(imputed_train$ximp, -TARGET_AMT))
summary(bk_elim_orig_vars <- backward_elimination(orig_var_glm))
glm_performance <- function(model) {
### Summarizes the model's key statistics
### References: https://www.r-bloggers.com/predicting-creditability-using-logistic-regression-in-r-cross-validating-the-classifier-part-2-2/
### https://www.rdocumentation.org/packages/boot/versions/1.3-20/topics/cv.glm
### https://www.rdocumentation.org/packages/ResourceSelection/versions/0.3-1/topics/hoslem.test
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
df_summary <- data.frame(
# model = bk_elim_orig_vars
model_name = deparse(substitute(model)),
n_vars = length(coef(model)) - 1,
model_pvalue = formatC(pchisq(model$null.deviance - model$deviance, 1, lower=FALSE), format = "e", digits = 2),
residual_deviance = model$deviance,
H_L_pvalue = ResourceSelection::hoslem.test(model$y, fitted(model))$p.value,
VIF_gt_4 = sum(car::vif(model) > 4),
CV_accuracy = 1 - boot::cv.glm(model$model, model, cost = cost, K = 100)$delta[1],
AUC = as.numeric(pROC::roc(model$y, fitted(model))$auc)
)
return(df_summary)
}
glm_models <- glm_performance(bk_elim_orig_vars)
transf_var_glm <- glm(TARGET_FLAG ~ . , family = "binomial", data = dplyr::select(train_transformed, -TARGET_AMT))
summary(bk_elim_transf_vars <- backward_elimination(transf_var_glm))
glm_models <- glm_performance(bk_elim_transf_vars)
glm_mod <- glm_performance(bk_elim_transf_vars)
glm_models <- glm_performance(bk_elim_orig_vars)
glm_models <- rbind(glm_models, glm_mod)
glm_models
bk_elim_transf_vars_roc <- roc(bk_elim_transf_vars$y, fitted(bk_elim_transf_vars))
plot(bk_elim_orig_vars_roc, legacy.axes = T, main = "Model ROCs", col = "gray", xaxs = "i", yaxs = "i")
plot(bk_elim_transf_vars_roc, add = T, col = "blue", lty = 3)
BIC_transf_var_glm <- step(transf_var_glm, k = log(n), trace = 0)
summary(BIC_transf_var_glm <- step(transf_var_glm, k = log(n), trace = 0))
glm_mod <- glm_performance(BIC_transf_var_glm)
glm_models <- rbind(glm_models, glm_mod)
glm_models
BIC_transf_var_glm
summary(BIC_transf_var_glm)
BIC_transf_var_glm_roc <- roc(BIC_transf_var_glm$y, fitted(BIC_transf_var_glm))
plot(bk_elim_orig_vars_roc, legacy.axes = T, main = "Model ROCs", col = "gray", xaxs = "i", yaxs = "i")
plot(bk_elim_transf_vars_roc, add = T, col = "blue", lty = 3)
plot(BIC_transf_var_glm_roc, add = T, col = "orange", lty = 3)
# 4. SELECT MODELS
glm_models <- as.character(all_results$model_name)
# 4. SELECT MODELS
all_models <- as.character(glm_models$model_name)
confusion_metrics <- data.frame(metric =  c("Accuracy", "Class. Error Rate", "Sensitivity", "Specificity", "Precision", "F1", "AUC"))
for (i in 1:length(all_models)){
model <- get(all_models[i])
model_name <- all_models[i]
predicted_values <- as.factor(as.integer(fitted(model) > .5))
CM <- confusionMatrix(predicted_values, as.factor(model$y), positive = "1")
caret_metrics <- c(CM$overall[1],
1 - as.numeric(CM$overall[1]),
CM$byClass[c(1, 2, 5, 7)],
get(paste0(model_name, "_roc"))$auc)
confusion_metrics[, model_name] <- caret_metrics
}
confusion_metrics_melted <- confusion_metrics %>%
reshape::melt(id.vars = "metric") %>%
dplyr::rename(model = variable)
ggplot(data = confusion_metrics_melted, aes(x = model, y = value)) +
geom_bar(aes(fill = model), stat='identity') +
theme(axis.ticks.x=element_blank(),
axis.text.x=element_blank(),
axis.title.x=element_blank()) +
facet_grid(~metric)
#https://stackoverflow.com/questions/18624394/ggplot-bar-plot-with-facet-dependent-order-of-categories/18625739?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
#https://stackoverflow.com/questions/18158461/grouped-bar-plot-in-ggplot?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
#kable(confusion_metrics, digits = 3)
unlink('D621-Data-Mining/HW4 Auto Accident Prediction/HW4 Auto Accident Prediction_cache', recursive = TRUE)
# influential leverage values
# MARR p291
hvalues <- influence(BIC_transf_var_glm)$hat
stanresDeviance <- residuals(BIC_transf_var_glm) / sqrt(1 - hvalues)
n_predictors <- length(names(BIC_transf_var_glm$model)) - 1
average_leverage <- (n_predictors + 1) / nrow(BIC_transf_var_glm$model)
plot(hvalues, stanresDeviance,
ylab = "Standardized Deviance Residuals",
xlab = "Leverage Values",
ylim = c(-3, 3),
xlim = c(-0.02, 0.05))
abline(v = 2 * average_leverage, lty = 2)
#Reference: http://www.stat.tamu.edu/~sheather/book/docs/rcode/Chapter8.R
# influential leverage values
# MARR p291
hvalues <- influence(BIC_transf_var_glm)$hat
stanresDeviance <- residuals(BIC_transf_var_glm) / sqrt(1 - hvalues)
n_predictors <- length(names(BIC_transf_var_glm$model)) - 1
average_leverage <- (n_predictors + 1) / nrow(BIC_transf_var_glm$model)
plot(hvalues, stanresDeviance,
ylab = "Standardized Deviance Residuals",
xlab = "Leverage Values",
ylim = c(-3, 3),
xlim = c(-0.02, 0.05))
abline(v = 2 * average_leverage, lty = 2)
#Reference: http://www.stat.tamu.edu/~sheather/book/docs/rcode/Chapter8.R
# influential leverage values
# MARR p291
hvalues <- influence(BIC_transf_var_glm)$hat
stanresDeviance <- residuals(BIC_transf_var_glm) / sqrt(1 - hvalues)
n_predictors <- length(names(BIC_transf_var_glm$model)) - 1
average_leverage <- (n_predictors + 1) / nrow(BIC_transf_var_glm$model)
plot(hvalues, stanresDeviance,
ylab = "Standardized Deviance Residuals",
xlab = "Leverage Values",
ylim = c(-3, 3),
xlim = c(-0.01, 0.02))
abline(v = 2 * average_leverage, lty = 2)
#Reference: http://www.stat.tamu.edu/~sheather/book/docs/rcode/Chapter8.R
# influential leverage values
# MARR p291
hvalues <- influence(BIC_transf_var_glm)$hat
stanresDeviance <- residuals(BIC_transf_var_glm) / sqrt(1 - hvalues)
n_predictors <- length(names(BIC_transf_var_glm$model)) - 1
average_leverage <- (n_predictors + 1) / nrow(BIC_transf_var_glm$model)
plot(hvalues, stanresDeviance,
ylab = "Standardized Deviance Residuals",
xlab = "Leverage Values",
ylim = c(-3, 3),
xlim = c(0, 0.02))
abline(v = 2 * average_leverage, lty = 2)
#Reference: http://www.stat.tamu.edu/~sheather/book/docs/rcode/Chapter8.R
# influential leverage values
# MARR p291
hvalues <- influence(BIC_transf_var_glm)$hat
stanresDeviance <- residuals(BIC_transf_var_glm) / sqrt(1 - hvalues)
n_predictors <- length(names(BIC_transf_var_glm$model)) - 1
average_leverage <- (n_predictors + 1) / nrow(BIC_transf_var_glm$model)
plot(hvalues, stanresDeviance,
ylab = "Standardized Deviance Residuals",
xlab = "Leverage Values",
ylim = c(-3, 3),
xlim = c(0, 0.15))
abline(v = 2 * average_leverage, lty = 2)
#Reference: http://www.stat.tamu.edu/~sheather/book/docs/rcode/Chapter8.R
# influential leverage values
# MARR p291
hvalues <- influence(BIC_transf_var_glm)$hat
stanresDeviance <- residuals(BIC_transf_var_glm) / sqrt(1 - hvalues)
n_predictors <- length(names(BIC_transf_var_glm$model)) - 1
average_leverage <- (n_predictors + 1) / nrow(BIC_transf_var_glm$model)
plot(hvalues, stanresDeviance,
ylab = "Standardized Deviance Residuals",
xlab = "Leverage Values",
ylim = c(-3, 3),
xlim = c(0, 0.015))
abline(v = 2 * average_leverage, lty = 2)
#Reference: http://www.stat.tamu.edu/~sheather/book/docs/rcode/Chapter8.R
names(marginal_model_plots)
if(!exists("marginal_model_plots"))
marginal_model_plots <- car::mmps(BIC_transf_var_glm)
