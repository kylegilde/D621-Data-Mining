---
title: "DATA 621 Business Analytics & Data Mining" 
subtitle: "Homework #3 Binary Logistic Regression"
author: "Kyle Gilde"
date: "3/31/2018"
output: 
  #pdf_document:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    toc_depth: 3
---


```{r knitr_options, echo=FALSE}

knitr::opts_chunk$set(
                      error = F
                      ,message = T
                      #,tidy = T
                      ,cache = T
                      , warning=F
                      , echo = F
                      )
```


```{r packages, echo=F, collapse=T} 

installed_and_loaded <- function(pkg){
  # Load packages. Install them if needed.
  # CODE SOURCE: https://gist.github.com/stevenworthington/3178163
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}


# requi/red packages
packages <- c("prettydoc","tidyverse", "caret", "pROC", "DT", "knitr", "ggthemes", "Hmisc", "psych", "corrplot", "reshape2", "car", "MASS", "ResourceSelection", "glmulti") 

#excute function and display the loaded packages
data.frame(installed_and_loaded(packages))
```
# Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0). 

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or, variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

# Deliverables

A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details. Assigned prediction (probabilities, classifications) for the evaluation data set. Use 0.5 threshold. Include your R statistical programming code in an Appendix.



# 1. DATA EXPLORATION

```{r getdata}
#import data

train_data <- read.csv("https://raw.githubusercontent.com/kylegilde/D621-Data-Mining/master/HW3%20Binary%20Logistic%20Regression/crime-training-data_modified.csv")

eval_data <- read.csv("https://raw.githubusercontent.com/kylegilde/D621-Data-Mining/master/HW3%20Binary%20Logistic%20Regression/crime-evaluation-data_modified.csv")


```



## Examine the data

* This data set was first published in 1978, and it contains 13 variables related to housing, property, transportation, geography, environment, education & crime for the Boston metropolitan area. 

* For this binary logistic regression, the response variable `target` is either a 1 or 0, where 1 indicates that the crime rate is above the median.

* Of the 12 explanatory variables, 11 are numeric, and only `chas` is categorical, which is a dummy variable indicating whether the suburb borders the Charles River

* The training data contains 466 complete cases while the evaluation data contains 40 complete case.


```{r meta, echo=F, fig.width = 10}

# train_data <- train_raw
# train_data$chas <- as.factor(train_data$chas)
# train_data$target <- as.factor(train_data$target)

metadata <- function(df){
  #Takes a data frame & Checks NAs, class types, inspects the unique values
  df_len <- nrow(df)
  NA_ct = as.vector(rapply(df, function(x) sum(is.na(x))))

  #create dataframe  
  df_metadata <- data.frame(
    vars = names(df),
    class_type = rapply(lapply(df, class), function(x) x[[1]]),
    n_rows = rapply(df, length),
    complete_cases = sum(complete.cases(df)),
    NA_ct = NA_ct,
    NA_pct = NA_ct / df_len * 100,
    unique_value_ct = rapply(df, function(x) length(unique(x))),
    most_common_values = rapply(df, function(x) str_replace(paste(names(sort(summary(as.factor(x)), decreasing=T))[1:5], collapse = '; '), "\\(Other\\); ", ""))
  )
 rownames(df_metadata) <- NULL
 return(df_metadata)
}

meta_df <- metadata(train_data)

datatable(meta_df, options = list(searching = F, paging = F)) 
```

```{r dims, results=F}
#dimensions
nrow(train_data)
nrow(eval_data)
sum(complete.cases(train_data))
sum(complete.cases(eval_data))
```

## Data Dictionary

![](https://raw.githubusercontent.com/kylegilde/D621-Data-Mining/master/HW3%20Binary%20Logistic%20Regression/data-dict.PNG)

If we employ the heuristic that crime-prone areas alre more likely to have less desirable characteristics, we would suspect that the crime rate might be **positively correlated** with the follow variables:

  - having industry (`indus`)
  - pollution (`nox`) 
  - pupil-teacher ratios (`ptratio`) 
  - lower social status (`lstat`)

Conversely, we would expect that the crime rate would be **inversely related** to the following variables: 

  + rate of large residential lots (`zn`) 
  + the average rooms per dwelling (`rm`)
  + access to radial highways (`rad`) 
  + median owner-occupied home values (`medv`)

Without knowing more about 1970s Boston, it's difficult to hypothesize on the relationship of the crime rate to following variables:

  + bordering the Charles River (`chas`)
  + having owner-occupied homedbuilt before 1940 (`age`)
  + the distance to Boston's employment centers (`dis`)
  + the property tax rate per $10,000. (`tax`)


##Statistical Summary

+ In the summary statistics below, the skewness and kurtosis of the varaibles are not large enough to suggest a transformation.

+ Besides the dummy variable for bording the Charles River `chas`, only `zn` has a heavier tail than a normal distribution, and it also has a decent amount of right skewness.


```{r stats, echo=F}

metrics <- function(df){
  ###Creates summary metrics table
  metrics_only <- df[, which(rapply(lapply(df, class), function(x) x[[1]]) %in% c("numeric", "integer"))]
  
  df_metrics <- psych::describe(metrics_only, quant = c(.25,.75))
  
  df_metrics <- 
    dplyr::select(df_metrics, n, min, Q_1st = Q0.25, median, mean, Q_3rd = Q0.75, 
    max, range, sd, skew, kurtosis
  )
  
  return(df_metrics)
}


metrics_df <- metrics(train_data)
datatable(round(metrics_df, 2), options = list(searching = F, paging = F))
#kable(metrics_df, digits = 1, format.args = list(big.mark = ',', scientific = F, drop0trailing = T))

```



## Visual Exploration

### Pairwise scatterplots

* Several of the predictor variables may have nonlinear relationships with each other. 

```{r scatter, fig.width = 11, fig.height = 11, echo=F}

pairs(train_data)

```


### Correlations

+ In the correlation plot and first table below it, we see that only 2 of the variables are highly correlated with each other. Having access to radial highways (`rad`) and the property tax rate per $10,000. (`tax`) have a positive correlation of 


+ In the second table, the concentration of nitrogen oxide (`nox`) has the highest correlation with the response variable. It has a positive correlation of 0.7261.

+ In our final model, we will want to make sure that we have eliminated multicollinearity by checking the variance inflation factorl

```{r fig.width = 11, fig.height = 11, echo=F}
##CORRELATIONS
cormatrix <- cor(train_data)

#plot
corrplot(cormatrix, method = "square", type = "upper")

#find the top correlations
correlations <- c(cormatrix[upper.tri(cormatrix)])
cor_df <- data.frame(Var1=rownames(cormatrix)[row(cormatrix)[upper.tri(cormatrix)]],
                     Var2=colnames(cormatrix)[col(cormatrix)[upper.tri(cormatrix)]],
                     Correlation=correlations,
                     Rsquared=correlations^2) %>% 
  arrange(-Rsquared)
#Reference: https://stackoverflow.com/questions/28035001/transform-correlation-matrix-into-dataframe-with-records-for-each-row-column-pai

#par(mfrow=c(1, 2))

kable(head(cor_df, 10), digits=4, row.names = T, caption = "Top Correlated Variable Pairs")

#Corrrelations with Target
target_corr <- subset(cor_df, Var2 == "target" | Var1 == "target")
rownames(target_corr) <- 1:nrow(target_corr)

kable(target_corr, digits=4, row.names = T, caption = "Corrrelations with the Response Variable")

```

### Histograms


+ The proportion of large residential lots `zn` is very skewed, and it may benefit from a  transformation.

+ To lesser degrees, `dis`, `age` & `lstat` are skewed. We will consider transforms for them as well.

```{r hist, fig.width = 11, fig.height = 11, echo=F}


#Predictor histograms
hist.data.frame(train_data)
#hist.data.frame(log(train_data))

```


### Side-by-Side Boxplots

+ The variance between the 2 values of target differs for `zn`, `nox`, `age`, `dis`, `rad` & `tax`, which indicates that we will want to consider adding quadratic terms for them.

```{r box, fig.width = 11, fig.height = 11}

# Side-by-Side Boxplots
train_melted <- melt(train_data, id.vars = "target") %>% 
  dplyr::filter(variable != "chas") %>% 
  mutate(target = as.factor(target))

ggplot(data = train_melted, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=target)) + 
  facet_wrap( ~ variable, scales="free")

#Reference: https://stackoverflow.com/questions/14604439/plot-multiple-boxplot-in-one-graph?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
```



# 2. DATA PREPARATION

+ Because the skewed distributions for dis`& `lstat, we will follow Sheather's quote of Cook & Weisberg on page 284 and add log terms to the model. 

+ Because of the variance between the 2 values of target differs for `zn`, `nox` & `rad`, we will follow Sheather's advice on page 289 and add quadratic terms to the model. 

+ For `zn`specifically, after examining the extreme difference in variances in the boxplot, adding a quadratic terms seems most appropriate.


```{r}

base_model <- glm(target ~ . , family = "binomial", data = train_data)


base_model_plus <- update(base_model, .~. + I(zn^2)  + I(rad^2) + I(nox^2) + log(age)  + log(lstat), data = train_data)

formula(base_model_plus)
 

#+ I(tax^2) +  log(dis)
```



# 3. Build Models

## Base model: All variables

First, let's take a look at a model that includes all of the variables.

+ Initially, five variables do not have statistically significant (stat-sig) p-values at a significance level of .05.

+ Contrary to what we suspected, the proporition of non-retal business `indus` has a negative coefficient, and having access to radial highways `rad` and median owner-occupied home values `medv` have actually have positive coefficients.

```{r base}

summary(base_model_plus)



```


## Backward Elimination



```{r mod1}


backward_elim_glm <- function(glmod){
  #performs backward elimination model selection 
  #removes variables until all remaining ones are stat-sig
  pvalues <- summary(glmod)$coefficients[,4]
  remove <- names(which.max(pvalues))
  removed_vars <- remove
  max_pvalue <- max(pvalues)
  removed_pvalues <- max_pvalue
  
  while (max_pvalue > .05){
    # remove the insignificant pvalue
    glmod <- update(glmod, as.formula(paste(".~.-", remove)))
    # see if there are any insignificant pvalues remaining
    pvalues <- summary(glmod)$coefficients[,4]
    max_pvalue <- max(pvalues)
    # if no, end loop     
    if (max_pvalue <= .05) break

    remove <- names(which.max(pvalues))
    removed_vars <- c(removed_vars, remove)
    removed_pvalues <- c(removed_pvalues, max_pvalue)   

  }
  
  print("Removed variables:")
  print(removed_vars)
  print(removed_pvalues)
  return(glmod)
}

summary(bk_elim_mod <- backward_elim_glm(base_model_plus))

sort(car::vif(bk_elim_mod), decreasing = T)


model_summary <- function(model) {
    ### Summarizes the model's key statistics in one row
    df_summary <- data.frame(
      model_name = deparse(substitute(model)),
      n_vars = length(coef(model)) - 1,
      pvalue = pchisq(model$null.deviance - model$deviance, 1, lower=FALSE),
      H_L_pvalue = hoslem.test(model$y, fitted(model))$p.value,
      VIF_gt_4 = sum(car::vif(model) > 4),
      AUC = pROC::roc(model$y, fitted(model))$auc
    )
}

#(curveROC <- pROC::roc(model$y, fitted(model)))


#df_summary$RootMSLE = MLmetrics::RMSLE(, )
mod_sum <- model_summary(bk_elim_mod)
kable(all_results <- mod_sum)

#References: https://www.rdocumentation.org/packages/ResourceSelection/versions/0.3-1/topics/hoslem.test
# http://www.stat.tamu.edu/~sheather/book/docs/rcode/Chapter8.R
```

##AIC

```{r mod2}

AIC_mod <- MASS::stepAIC(base_model_plus, trace = 0)
summary(AIC_mod)

#plot(mod2)
sort(vif(AIC_mod), decreasing = T)

mod_sum <- model_summary(AIC_mod)
kable(all_results <- rbind(all_results, mod_sum))

```

#BIC

```{r mod3}
n <- nrow(train_data)

BIC_mod <- step(base_model_plus, k = log(n), trace = 0)

summary(BIC_mod)

sort(vif(BIC_mod), decreasing = T)

cook_calcs <- cooks.distance(bk_elim_mod)
sum(cook_calcs>4*mean(cook_calcs))


mod_sum <- model_summary(BIC_mod)
kable(all_results <- rbind(all_results, mod_sum))


#Reference: https://stackoverflow.com/questions/19400494/running-a-stepwise-linear-model-with-bic-criterion?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
```

#glmulti Package & Function

To achieve this goal, glmulti clearly has to generate all possible model formulas that involve
some of the specified effects. More exactly, the user specifies which main effects are to be
considered, and the program will generate all models including some of these effects and,
optionally, their pairwise interactions.

```{r, mod4}

start <- Sys.time() 
glmulti_mod <- glmulti(base_model_plus, method = "g", maxsize = 12)
end_time <- Sys.time() 
duration <-  end_time - start


mod_sum <- model_summary(glmulti_mod)
kable(all_results <- rbind(all_results, glmulti_mod))


```

```{r}
hist(predict(BIC_mod, type = "response"))
hist(fitted(BIC_mod))

predicted_class <- as.factor(ifelse(fitted(BIC_mod) > .5, 1, 0))

(cMatrix <- confusionMatrix(predicted_class, as.factor(BIC_mod$y), positive = "1"))


```


#Code Appendix
```{r appendix, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```
