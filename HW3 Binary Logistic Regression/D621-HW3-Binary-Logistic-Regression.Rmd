---
title: "DATA 621 Business Analytics & Data Mining" 
subtitle: "Homework #3 Binary Logistic Regression"
author: "Kyle Gilde"
date: "3/31/2018"
output: 
  #pdf_document:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    toc_depth: 3
---


```{r knitr_options, echo=FALSE}

knitr::opts_chunk$set(
                      error = F
                      ,message = T
                      #,tidy = T
                      ,cache = T
                      , warning=F
                      , echo = F
                      )
```


```{r packages, echo=F, collapse=T} 

installed_and_loaded <- function(pkg){
  # Load packages. Install them if needed.
  # CODE SOURCE: https://gist.github.com/stevenworthington/3178163
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}

# requi/red packages
packages <- c("prettydoc","tidyverse", "caret", "pROC", "zoo", "DT", "knitr", "ggthemes", "Hmisc", "psych", "corrplot") 

#excute function and display the loaded packages
data.frame(installed_and_loaded(packages))
```
# Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0). 

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or, variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

# Deliverables

A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details. Assigned prediction (probabilities, classifications) for the evaluation data set. Use 0.5 threshold. Include your R statistical programming code in an Appendix.



# 1. DATA EXPLORATION

```{r getdata}
#import data

train_data <- read.csv("https://raw.githubusercontent.com/kylegilde/D621-Data-Mining/master/HW3%20Binary%20Logistic%20Regression/crime-training-data_modified.csv")

eval_data <- read.csv("https://raw.githubusercontent.com/kylegilde/D621-Data-Mining/master/HW3%20Binary%20Logistic%20Regression/crime-evaluation-data_modified.csv")


```



## Examine the data

* This data set was first published in 1978, and it contains 13 variables related to housing, property, transportation, geography, environment, education & crime for the Boston metropolitan area. 

* For this binary logistic regression, the response variable `target` is either a 1 or 0, where 1 indicates that the crime rate is above the median.

* Of the 12 explanatory variables, 11 are numeric, and only `chas` is categorical, which is a dummy variable indicating whether the suburb borders the Charles River

* The training data contains 466 complete cases while the evaluation data contains 40 complete case.

```{r dims, results=F}
#dimensions
nrow(train_data)
nrow(eval_data)
sum(complete.cases(train_data))
sum(complete.cases(eval_data))
```

* If we employ the heuristic that crime-prone areas alre more likely to have less desirable characteristics, we would suspect that the crime rate might be **positively correlated** with the follow variables:
  - having industry (`indus`)
  - pollution (`nox`) 
  - pupil-teacher ratios (`ptratio`) 
  - lower social status (`lstat`)

* Conversely, we would expect that the crime rate would be **inversely related** to the following variables: 
  + rate of large residential lots (`zn`) 
  + the average rooms per dwelling (`rm`)
  + access to radial highways (`rad`) 
  + median owner-occupied home values (`medv`)

* Without knowing more about 1970s Boston, it's difficult to hypothesize on the relationship of the crime rate to following variables:
  + bordering the Charles River (`chas`)
  + having owner-occupied homedbuilt before 1940 (`age`)
  + the distance to Boston's employment centers (`dis`)
  + the property tax rate per $10,000. (`tax`)


![](https://raw.githubusercontent.com/kylegilde/D621-Data-Mining/master/HW3%20Binary%20Logistic%20Regression/data-dict.PNG)


```{r meta, echo=F, fig.width = 10}

# train_data <- train_raw
# train_data$chas <- as.factor(train_data$chas)
# train_data$target <- as.factor(train_data$target)

metadata <- function(df){
  #Takes a data frame & Checks NAs, class types, inspects the unique values
  df_len <- nrow(df)
  NA_ct = as.vector(rapply(df, function(x) sum(is.na(x))))

  #create dataframe  
  df_metadata <- data.frame(
    vars = names(df),
    class_type = rapply(lapply(df, class), function(x) x[[1]]),
    n_rows = rapply(df, length),
    complete_cases = sum(complete.cases(df)),
    NA_ct = NA_ct,
    NA_pct = NA_ct / df_len * 100,
    unique_value_ct = rapply(df, function(x) length(unique(x))),
    most_common_values = rapply(df, function(x) str_replace(paste(names(sort(summary(as.factor(x)), decreasing=T))[1:5], collapse = '; '), "\\(Other\\); ", ""))
  )
 rownames(df_metadata) <- NULL
 return(df_metadata)
}

meta_df <- metadata(train_data)

datatable(meta_df, options = list(searching = F, paging = F)) 
```



##Statistical Summary

+ In the summary statistics below, the skewness and kurtosis of the varaibles are not large enough to suggest a transformation.

+ Only `zn` has heavier tails than a normal distribution, and it also has a decent amount of right skewness.

#only the rate of large residential lots (`zn`) has  skewness & k

```{r stats, echo=F}

metrics <- function(df){
  ###Creates summary metrics table
  metrics_only <- df[, which(rapply(lapply(df, class), function(x) x[[1]]) %in% c("numeric", "integer"))]
  
  df_metrics <- psych::describe(metrics_only, quant = c(.25,.75))
  
  df_metrics <- 
    dplyr::select(df_metrics, n, min, Q_1st = Q0.25, median, mean, Q_3rd = Q0.75, 
    max, range, sd, skew, kurtosis
  )
  
  return(df_metrics)
}


metrics_df <- metrics(train_data)
datatable(round(metrics_df, 2), options = list(searching = F, paging = F))
#kable(metrics_df, digits = 1, format.args = list(big.mark = ',', scientific = F, drop0trailing = T))

```



## Visual Exploration

### Boxplots & Histograms


+ The plots confirm the pronounced right skewness of `zn`

```{r plots, fig.width = 11, fig.height = 11, echo=F}
#calculate some parameters to deal with the outliers
train_stacked <- stack(train_data)

#boxplots
ggplot(data = train_stacked, mapping = aes(x = ind, y = values)) + 
  geom_boxplot() + 
  facet_wrap(~ind, scale="free", nrow = 7, ncol = 2) +
  labs(x="", y="", caption = "Red dot = mean") +
  stat_summary(fun.y=mean, geom="point", size=2, color = "red") +
  coord_flip() +
  theme_fivethirtyeight()

#histograms
hist.data.frame(train_data)

```

### Correlations

+ In the correlation table and plot below, we see 4 pairs of highly correlated variables. They are corresponding metrics for batting and pitching: home runs, walks, strike outs & hits (rows 1-4). These may present multicollinearity issues in our modeling.

+ Rows 8 to 11 show batting & pitching hits and walks are moderately correlated to our response variable. Let's consider using them for one our models.

```{r fig.width = 10, fig.height = 11, echo=F}
##CORRELATIONS
cormatrix <- cor(train_data)

#plot
corrplot(cormatrix, method = "square", type = "upper")

#find the top correlations
correlations <- c(cormatrix[upper.tri(cormatrix)])
cor_df <- data.frame(Var1=rownames(cormatrix)[row(cormatrix)[upper.tri(cormatrix)]],
                     Var2=colnames(cormatrix)[col(cormatrix)[upper.tri(cormatrix)]],
                     Correlation=correlations,
                     Rsquared=correlations^2) %>% 
  arrange(-Rsquared)
# https://stackoverflow.com/questions/28035001/transform-correlation-matrix-into-dataframe-with-records-for-each-row-column-pai

#par(mfrow=c(1, 2))

kable(head(cor_df, 10), digits=4, row.names = T, caption = "Top Correlated Variable Pairs")

#Corrrelations with Target
target_corr <- subset(cor_df, Var2 == "target" | Var1 == "target")
rownames(target_corr) <- 1:nrow(target_corr)

kable(target_corr, digits=4, row.names = T, caption = "Corrrelations with Target")



hist(log(train_data$lstat))
```

# 2. DATA PREPARATION

# 3. Build Models

## Backward Elimination


```{r mod1}
base_model <- glm(target ~ ., family = "binomial", data = train_data)

mod1 <- base_model
summary(base_model)
pvalues <- summary(base_model)$coefficients[,4]
(remove <- names(which.max(pvalues)))
removed_vars <- c(remove)
(max_pvalue <- max(pvalues))


while (max_pvalue > .05){
  mod1 <- update(mod1, as.formula(paste(".~.-", remove)))
  print(summary(mod1))
  pvalues <- summary(mod1)$coefficients[,4]
  print("Remove this one:")
  print(remove <- names(which.max(pvalues)))
  print(max_pvalue <- max(pvalues))
  removed_vars <- c(removed_vars, remove)
}
removed_vars
summary(mod1)


```


```{r mod2}
#mod1 <- glm(target ~ ., family = "binomial", train_data)

#summary(mod1)

#bestglm
```


#Code Appendix
```{r appendix, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```
